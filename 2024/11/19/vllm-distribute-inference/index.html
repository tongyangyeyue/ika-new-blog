

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/ika-new-blog/img/headPortrait.jpg">
  <link rel="icon" href="/ika-new-blog/img/headPortrait.jpg">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="IKA">
  <meta name="keywords" content="">
  
    <meta name="description" content="文章基于vllm v0.6.2版本  Link: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.06180v1 一、 张量并行(TP)与流水线并行(PP)参数vllm 默认采用Megatron-LM算法实现tensor parallel。参数设置  12--tensor-parallel-size tp_num--pipeline-parallel-size pp_num 123vllm">
<meta property="og:type" content="article">
<meta property="og:title" content="vllm-distribute-inference">
<meta property="og:url" content="https://tongyangyeyue.github.io/ika-new-blog/2024/11/19/vllm-distribute-inference/index.html">
<meta property="og:site_name" content="IKA">
<meta property="og:description" content="文章基于vllm v0.6.2版本  Link: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2309.06180v1 一、 张量并行(TP)与流水线并行(PP)参数vllm 默认采用Megatron-LM算法实现tensor parallel。参数设置  12--tensor-parallel-size tp_num--pipeline-parallel-size pp_num 123vllm">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tongyangyeyue.github.io/ika-new-blog/2024/11/19/vllm-distribute-inference/architecture.png">
<meta property="og:image" content="https://tongyangyeyue.github.io/ika-new-blog/2024/11/19/vllm-distribute-inference/backend1.png">
<meta property="og:image" content="https://tongyangyeyue.github.io/ika-new-blog/2024/11/19/vllm-distribute-inference/ray1.png">
<meta property="article:published_time" content="2024-11-19T12:19:22.000Z">
<meta property="article:modified_time" content="2025-05-04T12:54:28.029Z">
<meta property="article:author" content="IKA">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://tongyangyeyue.github.io/ika-new-blog/2024/11/19/vllm-distribute-inference/architecture.png">
  
  
  
  <title>vllm-distribute-inference - IKA</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/ika-new-blog/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/ika-new-blog/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/ika-new-blog/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"tongyangyeyue.github.io","root":"/ika-new-blog/","version":"1.9.8","typing":{"enable":true,"typeSpeed":70,"cursorChar":"","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null},"gtag":null,"tajs":null,"woyaola":null,"cnzz":null},"search_path":"/ika-new-blog/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/ika-new-blog/js/utils.js" ></script>
  <script  src="/ika-new-blog/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/ika-new-blog/">
      <strong>IKA</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/ika-new-blog/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/ika-new-blog/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/ika-new-blog/cusImage/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>cusImage</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/ika-new-blog/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="vllm-distribute-inference"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-11-19 20:19" pubdate>
          2024年11月19日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          2.8k 字
        
      </span>
    

    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">vllm-distribute-inference</h1>
            
            
              <div class="markdown-body">
                
                <p>文章基于vllm v0.6.2版本</p>
<img src="/ika-new-blog/2024/11/19/vllm-distribute-inference/architecture.png" srcset="/ika-new-blog/img/loading.gif" lazyload class="" title="img">
<p>Link: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06180v1">https://arxiv.org/pdf/2309.06180v1</a></p>
<h2 id="一、-张量并行-TP-与流水线并行-PP"><a href="#一、-张量并行-TP-与流水线并行-PP" class="headerlink" title="一、 张量并行(TP)与流水线并行(PP)"></a>一、 张量并行(TP)与流水线并行(PP)</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>vllm 默认采用Megatron-LM算法实现tensor parallel。<br>参数设置 </p>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ada"><span class="hljs-comment">--tensor-parallel-size tp_num</span><br><span class="hljs-comment">--pipeline-parallel-size pp_num</span><br></code></pre></td></tr></table></figure>
<figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ada">vllm serve  Qwen/Qwen2-<span class="hljs-number">0.5</span>B \ <br>    <span class="hljs-comment">--tensor-parallel-size 1 \ </span><br>    <span class="hljs-comment">--pipeline-parallel-size 3 </span><br></code></pre></td></tr></table></figure>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>如果当前节点有多个GPU，但GPU数量无法整切模型，那么可以使TP设置成1 并使用流水线并行(PP)来对模型进行切分。</p>
<blockquote style="color: gray; font-size: 0.9em;">
  There is one edge case: if the model fits in a single node with multiple GPUs, but the number of GPUs cannot divide the model size evenly, you can use pipeline parallelism, which splits the model along layers and supports uneven splits. In this case, the tensor parallel size should be 1 and the pipeline parallel size should be the number of GPUs.
</blockquote>
当前流水线并行只支持LLaMa, GPT2, Mixtral, Qwen, Qwen2和Nemotron格式的模型
<blockquote style="color: gray; font-size: 0.9em;">
Pipeline parallel is a beta feature. It is only supported for online serving as well as LLaMa, GPT2, Mixtral, Qwen, Qwen2, and Nemotron style models.
</blockquote>

<h2 id="二、Distributed-Executor-Backend"><a href="#二、Distributed-Executor-Backend" class="headerlink" title="二、Distributed-Executor-Backend"></a>二、Distributed-Executor-Backend</h2><h3 id="Distributed-executor-backend"><a href="#Distributed-executor-backend" class="headerlink" title="Distributed_executor_backend"></a>Distributed_executor_backend</h3><p>vllm服务运行有三种方式：</p>
<ul>
<li>Single GPU</li>
<li>Python multiprocessing</li>
<li>Ray<br>参数设置</li>
</ul>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-attr">--distributed_executor_backend</span> mp # mp、ray、<span class="hljs-attribute">none</span><br></code></pre></td></tr></table></figure>
<p>如果没设置distributed_executor_backend则默认采用如下规则</p>
<img src="/ika-new-blog/2024/11/19/vllm-distribute-inference/backend1.png" srcset="/ika-new-blog/img/loading.gif" lazyload class="" title="img">
<blockquote style="color: gray; font-size: 0.9em;">
Link : https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/config.py#L836
</blockquote>
world_size 并行世界的大小
world_size = pipeline_parallel_size * tensor_parallel_size

<h3 id="Ray"><a href="#Ray" class="headerlink" title="Ray"></a>Ray</h3><p>Ray 是一个开源分布式框架，用于扩展 AI 和 Python 应用程序（如机器学习）。它提供了用于并行处理的计算层，因此您无需成为分布式系统专家。Ray 使用以下组件最大限度地降低了运行分布式单个和端到端机器学习工作流的复杂性：</p>
<ul>
<li>用于常见机器学习任务（如数据预处理、分布式训练、超参数调整、强化学习和模型服务）的可扩展库。</li>
<li>用于并行化和扩展 Python 应用程序的 Python 分布式计算框架。</li>
<li>用于将 Ray 集群与现有工具和基础设施（如 Kubernetes、AWS、GCP 和 Azure）集成和部署的集成和实用程序。<br>哪些部分使用了Ray？<br>在vllm中<strong>Multi-Node GPU (尤其是PP)、XPU、TPU的多卡兼容中使用</strong></li>
</ul>
<img src="/ika-new-blog/2024/11/19/vllm-distribute-inference/ray1.png" srcset="/ika-new-blog/img/loading.gif" lazyload class="" title="img">
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/issues/6556
</blockquote>

<h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>  Ray中应用程序的主进程（注意不是Ray本身是python应用程序）负责提交任务到Ray集群，并协调任务的执行。它通常是应用程序的入口点，负责初始化Ray并与集群进行交互。</p>
<h4 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h4><p>  Ray集群中的工作节点，负责执行Driver提交的任务。每个Worker都可以执行多个任务，并且可以并行处理这些任务。Worker节点是无状态的，执行完任务后会将结果返回给driver</p>
<h4 id="Actor"><a href="#Actor" class="headerlink" title="Actor"></a>Actor</h4><blockquote style="color: gray; font-size: 0.9em;">
https://docs.ray.io/en/latest/ray-core/actors.html#actors
</blockquote>
  一种有状态的worker，用于部分运行数据的存储与运算。

<h4 id="Bundles"><a href="#Bundles" class="headerlink" title="Bundles"></a>Bundles</h4><blockquote style="color: gray; font-size: 0.9em;">
https://docs.ray.io/en/latest/ray-core/scheduling/placement-group.html#bundles
</blockquote>
  A bundle is a collection of “resources”. It could be a single resource, {"CPU": 1}, or a group of resources, {"CPU": 1, "GPU": 4}. A bundle is a unit of reservation for placement groups. “Scheduling a bundle” means we find a node that fits the bundle and reserve the resources specified by the bundle. A bundle must be able to fit on a single node on the Ray cluster. For example, if you only have an 8 CPU node, and if you have a bundle that requires {"CPU": 9}, this bundle cannot be scheduled.
  资源包声明，可以声明一个或多个资源组成一个资源包，用于Placement Group 

<h4 id="Placement-Group"><a href="#Placement-Group" class="headerlink" title="Placement Group"></a>Placement Group</h4><blockquote style="color: gray; font-size: 0.9em;">
https://docs.ray.io/en/latest/ray-core/scheduling/placement-group.html#key-concepts
</blockquote>
  A placement group reserves the resources from the cluster. The reserved resources can only be used by tasks or actors that use the PlacementGroupSchedulingStrategy.
  - Placement groups are represented by a list of bundles. For example, {"CPU": 1} * 4 means you’d like to reserve 4 bundles of 1 CPU (i.e., it reserves 4 CPUs).
  - Bundles are then placed according to the placement strategies across nodes on the cluster.
  - After the placement group is created, tasks or actors can be then scheduled according to the placement group and even on individual bundles.
  用于调度一组资源Bundles，分配给worker和actor。

<h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver">主: ray <span class="hljs-built_in">start</span> <span class="hljs-comment">--head --port=6379  vllm serve </span><br>worker : ray <span class="hljs-built_in">start</span> <span class="hljs-comment">--address=header:6379 </span><br>vllm serve <br></code></pre></td></tr></table></figure>

<h4 id="Multiprocessing"><a href="#Multiprocessing" class="headerlink" title="Multiprocessing"></a>Multiprocessing</h4><p>基于python中的Multiprocessing。其是基于进程的并行。<br>主要用于单节点多GPU的运行。</p>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>Executor是vllm对任务进行分配的核心。其决定任务如何进行分发以及worker如何运行。<br>如下是不同场景下对应的Executor</p>
<table>
<thead>
<tr>
<th>device_type\backend</th>
<th>mp</th>
<th>None</th>
<th>ray</th>
</tr>
</thead>
<tbody><tr>
<td>neuron</td>
<td>NeuronExecutor</td>
<td>NeuronExecutor</td>
<td>NeuronExecutor</td>
</tr>
<tr>
<td>TPU</td>
<td>不支持</td>
<td>TPUExecutor</td>
<td>RayTPUExecutor</td>
</tr>
<tr>
<td>openvino</td>
<td>OpenVINOExecutor</td>
<td>OpenVINOExecutor</td>
<td>OpenVINOExecutor</td>
</tr>
<tr>
<td>XPU</td>
<td>不支持（异步支持MultiprocessingXPUExecutorAsync）</td>
<td>XPUExecutor</td>
<td>RayXPUExecutor</td>
</tr>
<tr>
<td>GPU</td>
<td>MultiprocessingGPUExecutor（要求不能开启VLLM_USE_RAY_SPMD_WORKER）</td>
<td>GPUExecutor</td>
<td>RayGPUExecutor</td>
</tr>
<tr>
<td>CPU</td>
<td>CPUExecutor</td>
<td>CPUExecutor</td>
<td>CPUExecutor</td>
</tr>
</tbody></table>
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/engine/llm_engine.py#L490
</blockquote>

<h3 id="RayGPUExecutor"><a href="#RayGPUExecutor" class="headerlink" title="RayGPUExecutor"></a>RayGPUExecutor</h3><p>RayGPUExecutor主要负责在Ray上创建和管理worker。以及请求的分发</p>
<h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>初始化分为两部分，一部分是RayGPUExecutor本身的变量设置_init_executor。 一部分是对worker的初始化设置_init_workers_ray<br>_init_executor主要功能是设置如下参数</p>
<ul>
<li>use_ray_spmd_worker</li>
<li>use_ray_compiled_dag</li>
<li>placement_group</li>
<li>RAY_USAGE_STATS_ENABLED</li>
<li>input_encoder</li>
<li>output_decoder<br>其中，比较重要的外部参数是use_ray_compiled_dag和use_ray_spmd_worker</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># If the env var is set, it uses the Ray&#x27;s compiled DAG API</span><br><span class="hljs-comment"># which optimizes the control plane overhead(开销).</span><br><span class="hljs-comment"># Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.</span><br><span class="hljs-comment"># Currently, this requires USE_RAY_SPMD_WORKER=True.</span><br><span class="hljs-variable language_">self</span>.use_ray_compiled_dag = envs.VLLM_USE_RAY_COMPILED_DAG<br><span class="hljs-comment"># If the env var is set, then we do not distinguish(区分) between the</span><br><span class="hljs-comment"># &quot;driver worker&quot; vs other workers. Also, the rank 0 worker will</span><br><span class="hljs-comment"># be executed in a remote Ray worker. Currently this requires</span><br><span class="hljs-comment"># USE_RAY_COMPILED_DAG=True.</span><br><span class="hljs-variable language_">self</span>.use_ray_spmd_worker = envs.VLLM_USE_RAY_SPMD_WORKER<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_ray_compiled_dag:<br>    <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker, (<br>        <span class="hljs-string">&quot;VLLM_USE_RAY_COMPILED_DAG=1 requires &quot;</span><br>        <span class="hljs-string">&quot;VLLM_USE_RAY_SPMD_WORKER=1&quot;</span>)<br><span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker:<br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Support SPMD worker for non-DAG Ray executor.</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.use_ray_compiled_dag, (<br>        <span class="hljs-string">&quot;VLLM_USE_RAY_SPMD_WORKER=1 requires &quot;</span><br>        <span class="hljs-string">&quot;VLLM_USE_RAY_COMPILED_DAG=1&quot;</span>)<br></code></pre></td></tr></table></figure>
<p><strong>SPMD （Single-Program Multiple-Data）</strong> 并行计算算法， 用于将数据切分<br><strong>DAG（Directed Acyclic Graph)</strong> 有向无环图  Ray 会将这些任务及其依赖关系构造成一个 DAG。在内部，Ray 会将用户定义的 DAG 编译成 COMPILED_DAG，然后交给调度器去处理。<br>use_ray_compiled_dag开启后会使用Ray’s compiled DAG的API，会节省ray控制面分发的开销，需要配合use_ray_spmd_worker一起开启<br>use_ray_spmd_worker开启后不会区分Driver和worker 需要和use_ray_compiled_dag 一起开启<br>开启方式：</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">RAY_COMPILED_DAG</span>=1<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">VLLM_USE_RAY_SPMD_WORKER</span>=1 <br><span class="hljs-built_in">export</span> <span class="hljs-attribute">VLLM_USE_RAY_COMPILED_DAG</span>=1<br></code></pre></td></tr></table></figure>
<p>具体来说，我们从驱动程序中删除了参数准备和模型执行功能，并使所有工作器都采用 SPMD 风格：LLMEngine&#x2F;驱动程序现在通过 Ray DAG 通道（共享内存）将输入传递给所有 SPMD 工作器，每个工作器准备参数并执行其模型分片。结果也通过 Ray DAG 通道传回Driver程序。</p>
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/issues/6556
</blockquote>

<p>_init_workers_ray主要功能是创建和分配worker和一些参数</p>
<ul>
<li>设置gpu_memory_utilization，如果tp和pp都为1的话才生效，否则每个worker为1</li>
<li>配置ray_workers_use_nsight</li>
<li>根据placement_group创建worker</li>
<li>将worker根据是否use_ray_spmd_worker来加入到不同列表中</li>
<li>初始化worker并load model</li>
<li>对worker进行分组</li>
</ul>
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/executor/ray_gpu_executor.py#L109
</blockquote>

<p>根据<strong>placement_group</strong>创建worker，如果开启了spmd，则不需要声明driver worker</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> bundle_id, bundle <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(placement_group.bundle_specs):<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> bundle.get(<span class="hljs-string">&quot;GPU&quot;</span>, <span class="hljs-number">0</span>): <span class="hljs-comment">#如果没GPU则跳过</span><br>        <span class="hljs-keyword">continue</span><br>    scheduling_strategy = PlacementGroupSchedulingStrategy(  <span class="hljs-comment"># 使用按PlacementGroup的调度策略</span><br>        placement_group=placement_group,<br>        placement_group_capture_child_tasks=<span class="hljs-literal">True</span>,<br>        placement_group_bundle_index=bundle_id,<br>    )<br><br>    worker = ray.remote(<br>        num_cpus=<span class="hljs-number">0</span>,<br>        num_gpus=num_gpus,<br>        scheduling_strategy=scheduling_strategy,<br>        **ray_remote_kwargs,<br>    )(RayWorkerWrapper).remote(**worker_wrapper_kwargs) <span class="hljs-comment"># 使用ray创建worker </span><br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker:<br>        <span class="hljs-variable language_">self</span>.workers.append(worker)   <span class="hljs-comment"># 如果是use_ray_spmd_worker 则不需要区分driver</span><br>    <span class="hljs-keyword">else</span>:<br>        worker_ip = ray.get(worker.get_node_ip.remote())<br>        <span class="hljs-keyword">if</span> worker_ip == driver_ip <span class="hljs-keyword">and</span> <span class="hljs-variable language_">self</span>.driver_dummy_worker <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># If the worker is on the same node as the driver, we use it</span><br>            <span class="hljs-comment"># as the resource holder for the driver process.</span><br>            <span class="hljs-variable language_">self</span>.driver_dummy_worker = worker        <span class="hljs-comment"># 需要一个和driver在同一节点的worker  只有get_node_and_gpu_ids用到 </span><br>            <span class="hljs-variable language_">self</span>.driver_worker = RayWorkerWrapper(   <span class="hljs-comment"># driver worker独立创建</span><br>                **worker_wrapper_kwargs)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Else, added to the list of workers.</span><br>            <span class="hljs-variable language_">self</span>.workers.append(worker)<br></code></pre></td></tr></table></figure>
<h4 id="execute-model"><a href="#execute-model" class="headerlink" title="execute_model"></a>execute_model</h4><blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/executor/ray_gpu_executor.py#L315
</blockquote>
llm_engine同步方式 使用模型不支持流水线并行（pp），所以调用execute_model直接使用driver_worker进行推理。所以猜测同步方式推理的话 ，推理只在driver_worker上运行，并不会到ray中的其他节点中去。


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_driver_execute_model</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self, execute_model_req: <span class="hljs-type">Optional</span>[ExecuteModelRequest]</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[SamplerOutput]]:<br>    <span class="hljs-string">&quot;&quot;&quot;Run execute_model in the driver worker.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Passing None will cause the driver to stop the model execution</span><br><span class="hljs-string">    loop running in each of the remote workers.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker, (<br>        <span class="hljs-string">&quot;driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1&quot;</span>)<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.driver_worker.execute_method(<span class="hljs-string">&quot;execute_model&quot;</span>,<br>                                             execute_model_req)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">execute_model</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        execute_model_req: ExecuteModelRequest</span>) -&gt; <span class="hljs-type">List</span>[SamplerOutput]:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker:<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">super</span>().execute_model(execute_model_req)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.forward_dag <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.forward_dag = <span class="hljs-variable language_">self</span>._compiled_ray_dag(enable_asyncio=<span class="hljs-literal">False</span>)<br><br>    serialized_data = <span class="hljs-variable language_">self</span>.input_encoder.encode(execute_model_req)<br>    outputs = ray.get(<span class="hljs-variable language_">self</span>.forward_dag.execute(serialized_data))<br>    output = <span class="hljs-variable language_">self</span>.output_decoder.decode(outputs[<span class="hljs-number">0</span>])<br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure>

<p>async_llm_engine方式则异步的对任务请求分发到每个节点去异步执行，并gather结果并返回</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">execute_model_async</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        execute_model_req: ExecuteModelRequest</span>) -&gt; <span class="hljs-type">List</span>[SamplerOutput]:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker:<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> <span class="hljs-built_in">super</span>().execute_model_async(execute_model_req)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.forward_dag <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-variable language_">self</span>.forward_dag = <span class="hljs-variable language_">self</span>._compiled_ray_dag(enable_asyncio=<span class="hljs-literal">True</span>)<br><br>    serialized_data = <span class="hljs-variable language_">self</span>.input_encoder.encode(execute_model_req)<br>    dag_future = <span class="hljs-keyword">await</span> <span class="hljs-variable language_">self</span>.forward_dag.execute_async(serialized_data)<br>    outputs = <span class="hljs-keyword">await</span> dag_future<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.output_decoder.decode(outputs[<span class="hljs-number">0</span>])<br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_driver_execute_model_async</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    execute_model_req: <span class="hljs-type">Optional</span>[ExecuteModelRequest] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">List</span>[SamplerOutput]:<br>    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.use_ray_spmd_worker, (<br>        <span class="hljs-string">&quot;driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1&quot;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.tp_driver_workers:<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> <span class="hljs-variable language_">self</span>.driver_exec_method(<span class="hljs-string">&quot;execute_model&quot;</span>,<br>                                             execute_model_req)<br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.pp_locks <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># This locks each pipeline parallel stage so multiple virtual</span><br>        <span class="hljs-comment"># engines can&#x27;t execute on the same stage at the same time</span><br>        <span class="hljs-comment"># We create the locks here to avoid creating them in the constructor</span><br>        <span class="hljs-comment"># which uses a different asyncio loop.</span><br>        <span class="hljs-variable language_">self</span>.pp_locks = [<br>            asyncio.Lock()<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.parallel_config.pipeline_parallel_size)<br>        ]<br><br>    tasks = [<br>        asyncio.create_task(<br>            _run_task_with_lock(<span class="hljs-variable language_">self</span>.driver_exec_method, <span class="hljs-variable language_">self</span>.pp_locks[<span class="hljs-number">0</span>],<br>                                <span class="hljs-string">&quot;execute_model&quot;</span>, execute_model_req))<br>    ]<br>    <span class="hljs-keyword">for</span> pp_rank, driver_worker <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.tp_driver_workers,<br>                                            start=<span class="hljs-number">1</span>):<br>        tasks.append(<br>            asyncio.create_task(<br>                _run_task_with_lock(driver_worker.execute_method.remote,<br>                                    <span class="hljs-variable language_">self</span>.pp_locks[pp_rank],<br>                                    <span class="hljs-string">&quot;execute_model&quot;</span>, execute_model_req)))<br><br>    results = <span class="hljs-keyword">await</span> asyncio.gather(*tasks)<br><br>    <span class="hljs-comment"># Only the last PP stage has the final results.</span><br>    <span class="hljs-keyword">return</span> results[-<span class="hljs-number">1</span>]<br><br></code></pre></td></tr></table></figure>
<h4 id="Ray-DAG"><a href="#Ray-DAG" class="headerlink" title="Ray DAG"></a>Ray DAG</h4><p>编译Ray DAG</p>
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/executor/ray_gpu_executor.py#L460
</blockquote>
- 创建DAG的起点
- 根据tp的数量生产一个列表
- 为每个tp组的worker绑定outputs的输入 ，并且上个pp组的worker的输出为下个pp组的输入
- 给每个output转换为指定类型
- 将 outputs放入到MultiOutputNode中准备编译
- 编译依赖生成有向无环图

<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs clean">def _compiled_ray_dag(self, enable_asyncio: bool):<br>    assert self.parallel_config.use_ray<br>    self._check_ray_adag_installation()<br>    <span class="hljs-keyword">from</span> ray.dag <span class="hljs-keyword">import</span> InputNode, MultiOutputNode<br>    <span class="hljs-keyword">from</span> ray.experimental.channel.torch_tensor_type <span class="hljs-keyword">import</span> TorchTensorType<br><br>    logger.info(<span class="hljs-string">&quot;VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL = %s&quot;</span>,<br>                envs.VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL)<br>    <span class="hljs-keyword">with</span> InputNode() <span class="hljs-keyword">as</span> input_data:  # DAG的起点<br>        # Example DAG: PP=<span class="hljs-number">2</span>, TP=<span class="hljs-number">4</span><br>        # (ExecuteModelReq, None) -&gt; <span class="hljs-number">0</span> -&gt; (ExecuteModelReq, IntermediateOutput) -&gt; <span class="hljs-number">4</span> -&gt; SamplerOutput   # noqa: E501<br>        #                         -&gt; <span class="hljs-number">1</span> -&gt; (ExecuteModelReq, IntermediateOutput) -&gt; <span class="hljs-number">5</span> -&gt; SamplerOutput   # noqa: E501<br>        #                         -&gt; <span class="hljs-number">2</span> -&gt; (ExecuteModelReq, IntermediateOutput) -&gt; <span class="hljs-number">6</span> -&gt; SamplerOutput   # noqa: E501<br>        #                         -&gt; <span class="hljs-number">3</span> -&gt; (ExecuteModelReq, IntermediateOutput) -&gt; <span class="hljs-number">7</span> -&gt; SamplerOutput   # noqa: E501<br><br>        # All workers <span class="hljs-keyword">in</span> the first TP group will take <span class="hljs-keyword">in</span> the<br>        # ExecuteModelRequest <span class="hljs-keyword">as</span> input.<br>        outputs = [input_data for _ <span class="hljs-keyword">in</span> self.pp_tp_workers[<span class="hljs-number">0</span>]]   #根据tp的数量，生成一个列表，列表的元素是input_data<br>        for pp_rank, tp_group <span class="hljs-keyword">in</span> enumerate(self.pp_tp_workers):  # 遍历pp组<br>            # Each PP worker takes <span class="hljs-keyword">in</span> the output <span class="hljs-keyword">of</span> the previous PP worker,<br>            # and the TP group executes <span class="hljs-keyword">in</span> SPMD fashion.<br>            outputs = [<br>                worker.execute_model_spmd.<br>                bind(  # type: ignore[attr-defined]<br>                    outputs[i]) for i, worker <span class="hljs-keyword">in</span> enumerate(tp_group)  # tp组里面的每个worker都赋予对应的input_data<br>            ]  # 上一组的输出，作为下一组的输入<br><br>            last_pp_rank = len(self.pp_tp_workers) - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> pp_rank &lt; last_pp_rank:<br>                # Specify how intermediate tensors should be passed<br>                # between pp stages, no need to specify for the last<br>                # pp stage.<br>                # 如果开启了VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL 则用nccl<br>                transport = <span class="hljs-string">&quot;nccl&quot;</span> \<br>                    <span class="hljs-keyword">if</span> envs.VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL \<br>                    else <span class="hljs-string">&quot;auto&quot;</span><br>                outputs = [<br>                    output.with_type_hint(<br>                        TorchTensorType(transport=transport))   # 给每个output转换为指定类型<br>                    for output <span class="hljs-keyword">in</span> outputs<br>                ]<br><br>        forward_dag = MultiOutputNode(outputs)  #将 outputs放入到MultiOutputNode中准备编译<br><br>    return forward_dag.experimental_compile(enable_asyncio=enable_asyncio)  # 根据任务的依赖关系，编译生成一个可执行的有向无环图结构<br></code></pre></td></tr></table></figure>
<p>worker以spmd方式执行</p>
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/worker/worker_base.py#L356
</blockquote>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_execute_model_spmd</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    execute_model_req: ExecuteModelRequest,</span><br><span class="hljs-params">    intermediate_tensors: <span class="hljs-type">Optional</span>[IntermediateTensors] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[SamplerOutput]]:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Execute model in Single Program Multiple Data (SPMD) fashion.</span><br><span class="hljs-string">    All workers take the same request, prepare the input and</span><br><span class="hljs-string">    execute the model.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">assert</span> execute_model_req <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>, (<br>        <span class="hljs-string">&quot;_execute_model_spmd() requires each worker to take in an &quot;</span><br>        <span class="hljs-string">&quot;ExecuteModelRequest&quot;</span>)<br>    worker_input: WorkerInput = <span class="hljs-variable language_">self</span>.prepare_worker_input(<br>        execute_model_req=execute_model_req)<br>    model_input: ModelRunnerInputBase = (<br>        <span class="hljs-variable language_">self</span>.model_runner.prepare_model_input(<br>            execute_model_req.seq_group_metadata_list))<br><br>    <span class="hljs-variable language_">self</span>.execute_worker(worker_input)<br><br>    <span class="hljs-comment"># If there is no input, we don&#x27;t need to execute the model.</span><br>    <span class="hljs-keyword">if</span> worker_input.num_seq_groups == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> []<br><br>    kwargs = extract_previous_hidden_states(execute_model_req)<br><br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.model_runner.execute_model(<br>        model_input=model_input,<br>        kv_caches=<span class="hljs-variable language_">self</span>.kv_cache[worker_input.virtual_engine]<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.kv_cache <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>        intermediate_tensors=intermediate_tensors,<br>        **kwargs,<br>    )<br></code></pre></td></tr></table></figure>

<h3 id="MultiprocessingGPUExecutor"><a href="#MultiprocessingGPUExecutor" class="headerlink" title="MultiprocessingGPUExecutor"></a>MultiprocessingGPUExecutor</h3><p>MultiprocessingGPUExecutor主要负责单节点多GPU上的worker执行。</p>
<h4 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h4><ul>
<li>设置OMP_NUM_THREADS</li>
<li>设置CacheManager</li>
<li>初始化workers、tp_driver_workers、non_driver_workers</li>
<li>初始化worker_monitor</li>
<li>创建driver_worker</li>
<li>init_device</li>
<li>load_model<br>参数： OMP_NUM_THREADS</li>
</ul>
<blockquote style="color: gray; font-size: 0.9em;">
https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp
</blockquote>
用于控制 OpenMP 并行库在执行并行代码时使用的线程数量。默认为1 。增大后会影响并行程序的cpu争抢，可以按需调整 

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> world_size == <span class="hljs-number">1</span>:<br>    <span class="hljs-variable language_">self</span>.worker_monitor = <span class="hljs-literal">None</span>   <span class="hljs-comment"># 如果只有一个worker，则不需要管理</span><br><span class="hljs-keyword">else</span>:<br>    result_handler = ResultHandler()<br>    <span class="hljs-keyword">for</span> rank <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, world_size):<br>        worker = ProcessWorkerWrapper(<br>            result_handler,<br>            partial(<br>                create_worker,<br>                **<span class="hljs-variable language_">self</span>._get_create_worker_kwargs(<br>                    rank=rank,<br>                    local_rank=rank,<br>                    distributed_init_method=distributed_init_method,<br>                )))<br>        <span class="hljs-variable language_">self</span>.workers.append(worker) <span class="hljs-comment"># 全部worker (tp_driver_workers + non_driver_workers 但不包含主程序)</span><br>        <span class="hljs-keyword">if</span> rank % tensor_parallel_size == <span class="hljs-number">0</span>:<br>            <span class="hljs-variable language_">self</span>.tp_driver_workers.append(worker) <span class="hljs-comment"># 比如一共8卡，tp = 2  那这个列表就是 2,4,6   (rank 序号从0开始)</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.non_driver_workers.append(worker)  <span class="hljs-comment"># 比如一共8卡，tp = 2  那这个列表就是 1,3,5,7  (rank 序号从0开始)</span><br><br>    <span class="hljs-variable language_">self</span>.worker_monitor = WorkerMonitor(<span class="hljs-variable language_">self</span>.workers, result_handler) <span class="hljs-comment"># 用于管理多个worker</span><br>    result_handler.start()<br>    <span class="hljs-variable language_">self</span>.worker_monitor.start()<br>    <br>    <span class="hljs-comment"># Set up signal handlers to shutdown the executor cleanly</span><br>    <span class="hljs-comment"># sometimes gc does not work well</span><br><br><span class="hljs-variable language_">self</span>.driver_worker = <span class="hljs-variable language_">self</span>._create_worker(<br>    distributed_init_method=distributed_init_method)  <span class="hljs-comment"># 当前主程序worker， 不在worker列表内</span><br><span class="hljs-variable language_">self</span>._run_workers(<span class="hljs-string">&quot;init_device&quot;</span>)<br><span class="hljs-variable language_">self</span>._run_workers(<span class="hljs-string">&quot;load_model&quot;</span>,<br>                  max_concurrent_workers=<span class="hljs-variable language_">self</span>.parallel_config.<br>                  max_parallel_loading_workers)<br></code></pre></td></tr></table></figure>
<h4 id="execute-model-1"><a href="#execute-model-1" class="headerlink" title="execute_model"></a>execute_model</h4><p>同步方法直接交给driver_worker执行<br>异步方法 和Ray几乎一样 交给每个driver_worker进行执行</p>
<blockquote style="color: gray; font-size: 0.9em;">
https://github.com/vllm-project/vllm/blob/v0.6.2/vllm/executor/multiproc_gpu_executor.py#L140
</blockquote>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_driver_execute_model</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self, execute_model_req: <span class="hljs-type">Optional</span>[ExecuteModelRequest]</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[SamplerOutput]]:<br>    <span class="hljs-string">&quot;&quot;&quot;Run execute_model in the driver worker.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Passing None will cause the driver to stop the model execution</span><br><span class="hljs-string">    loop running in each of the remote workers.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.driver_worker.execute_model(execute_model_req)<br><br><span class="hljs-keyword">async</span> <span class="hljs-keyword">def</span> <span class="hljs-title function_">_driver_execute_model_async</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    execute_model_req: <span class="hljs-type">Optional</span>[ExecuteModelRequest] = <span class="hljs-literal">None</span></span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-type">List</span>[SamplerOutput]:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.tp_driver_workers:<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">await</span> <span class="hljs-variable language_">self</span>.driver_exec_model(execute_model_req)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.pp_locks <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># This locks each pipeline parallel stage so multiple virtual</span><br>        <span class="hljs-comment"># engines can&#x27;t execute on the same stage at the same time</span><br>        <span class="hljs-comment"># We create the locks here to avoid creating them in the constructor</span><br>        <span class="hljs-comment"># which uses a different asyncio loop.</span><br>        <span class="hljs-variable language_">self</span>.pp_locks = [<br>            asyncio.Lock()<br>            <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.parallel_config.pipeline_parallel_size)<br>        ]<br><br>    tasks = [<br>        asyncio.create_task(<br>            _run_task_with_lock(<span class="hljs-variable language_">self</span>.driver_exec_model, <span class="hljs-variable language_">self</span>.pp_locks[<span class="hljs-number">0</span>],<br>                                execute_model_req))<br>    ]<br>    <span class="hljs-keyword">for</span> pp_rank, driver_worker <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.tp_driver_workers,<br>                                            start=<span class="hljs-number">1</span>):<br>        tasks.append(<br>            asyncio.create_task(<br>                _run_task_with_lock(driver_worker.execute_method_async,<br>                                    <span class="hljs-variable language_">self</span>.pp_locks[pp_rank],<br>                                    <span class="hljs-string">&quot;execute_model&quot;</span>, execute_model_req)))<br>    results = <span class="hljs-keyword">await</span> asyncio.gather(*tasks)<br><br>    <span class="hljs-comment"># Only the last PP stage has the final results.</span><br>    <span class="hljs-keyword">return</span> results[-<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure>

<h4 id="ProcessWorkerWrapper"><a href="#ProcessWorkerWrapper" class="headerlink" title="ProcessWorkerWrapper"></a>ProcessWorkerWrapper</h4><p>multiProcessing的worker执行引入了队列，任务在执行时会将task_id和任务参数进入队列中，并在_run_worker_process中执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ProcessWorkerWrapper</span>:<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, result_handler: ResultHandler,</span><br><span class="hljs-params">             worker_factory: <span class="hljs-type">Callable</span>[[], <span class="hljs-type">Any</span>]</span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-variable language_">self</span>._task_queue = mp.Queue()<br>    <span class="hljs-variable language_">self</span>.result_queue = result_handler.result_queue<br>    <span class="hljs-variable language_">self</span>.tasks = result_handler.tasks<br>    <span class="hljs-variable language_">self</span>.process: BaseProcess = mp.Process(  <span class="hljs-comment"># type: ignore[attr-defined]</span><br>        target=_run_worker_process,<br>        name=<span class="hljs-string">&quot;VllmWorkerProcess&quot;</span>,<br>        kwargs=<span class="hljs-built_in">dict</span>(<br>            worker_factory=worker_factory,<br>            task_queue=<span class="hljs-variable language_">self</span>._task_queue,<br>            result_queue=<span class="hljs-variable language_">self</span>.result_queue,<br>        ),<br>        daemon=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-variable language_">self</span>.process.start()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_enqueue_task</span>(<span class="hljs-params">self, future: <span class="hljs-type">Union</span>[ResultFuture, asyncio.Future],</span><br><span class="hljs-params">                  method: <span class="hljs-built_in">str</span>, args, kwargs</span>):<br>    task_id = uuid.uuid4()<br>    <span class="hljs-variable language_">self</span>.tasks[task_id] = future<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-variable language_">self</span>._task_queue.put((task_id, method, args, kwargs))<br>    <span class="hljs-keyword">except</span> SystemExit:<br>        <span class="hljs-keyword">raise</span><br>    <span class="hljs-keyword">except</span> BaseException <span class="hljs-keyword">as</span> e:<br>        <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.tasks[task_id]<br>        <span class="hljs-keyword">raise</span> ChildProcessError(<span class="hljs-string">&quot;worker died&quot;</span>) <span class="hljs-keyword">from</span> e<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">execute_method</span>(<span class="hljs-params">self, method: <span class="hljs-built_in">str</span>, *args, **kwargs</span>):<br>    future: ResultFuture = ResultFuture()<br>    <span class="hljs-variable language_">self</span>._enqueue_task(future, method, args, kwargs)<br>    <span class="hljs-keyword">return</span> future<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_run_worker_process</span>(<span class="hljs-params"></span><br><span class="hljs-params">    worker_factory: <span class="hljs-type">Callable</span>[[], <span class="hljs-type">Any</span>],</span><br><span class="hljs-params">    task_queue: Queue,</span><br><span class="hljs-params">    result_queue: Queue,</span><br><span class="hljs-params"></span>) -&gt; <span class="hljs-literal">None</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;Worker process event loop&quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># Add process-specific prefix to stdout and stderr</span><br>    process_name = mp.current_process().name<br>    pid = os.getpid()<br>    _add_prefix(sys.stdout, process_name, pid)<br>    _add_prefix(sys.stderr, process_name, pid)<br><br>    <span class="hljs-comment"># Initialize worker</span><br>    worker = worker_factory()<br>    <span class="hljs-keyword">del</span> worker_factory<br><br>    <span class="hljs-comment"># Accept tasks from the engine in task_queue</span><br>    <span class="hljs-comment"># and return task output in result_queue</span><br>    logger.info(<span class="hljs-string">&quot;Worker ready; awaiting tasks&quot;</span>)<br>    <span class="hljs-keyword">try</span>:<br>        <span class="hljs-keyword">for</span> items <span class="hljs-keyword">in</span> <span class="hljs-built_in">iter</span>(task_queue.get, _TERMINATE):<br>            output = <span class="hljs-literal">None</span><br>            exception = <span class="hljs-literal">None</span><br>            task_id, method, args, kwargs = items<br>            <span class="hljs-keyword">try</span>:<br>                executor = <span class="hljs-built_in">getattr</span>(worker, method)<br>                output = executor(*args, **kwargs)<br>            <span class="hljs-keyword">except</span> SystemExit:<br>                <span class="hljs-keyword">raise</span><br>            <span class="hljs-keyword">except</span> KeyboardInterrupt:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">except</span> BaseException <span class="hljs-keyword">as</span> e:<br>                tb = traceback.format_exc()<br>                logger.error(<br>                    <span class="hljs-string">&quot;Exception in worker %s while processing method %s: %s, %s&quot;</span>,<br>                    process_name, method, e, tb)<br>                exception = e<br>            result_queue.put(<br>                Result(task_id=task_id, value=output, exception=exception))<br>    <span class="hljs-keyword">except</span> KeyboardInterrupt:<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">except</span> Exception:<br>        logger.exception(<span class="hljs-string">&quot;Worker failed&quot;</span>)<br><br>    logger.info(<span class="hljs-string">&quot;Worker exiting&quot;</span>)<br></code></pre></td></tr></table></figure>

<h2 id="可调整参数"><a href="#可调整参数" class="headerlink" title="可调整参数"></a>可调整参数</h2><ul>
<li>distributed_executor_backend 分布式推理方式，可选值: mp、ray、none</li>
<li>是否开启SPMD和DAG，数据并行与Ray有向无环图编译</li>
</ul>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">RAY_COMPILED_DAG</span>=1<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">VLLM_USE_RAY_SPMD_WORKER</span>=1 <br><span class="hljs-built_in">export</span> <span class="hljs-attribute">VLLM_USE_RAY_COMPILED_DAG</span>=1<br></code></pre></td></tr></table></figure>
<ul>
<li>OMP_NUM_THREADS 默认为1  ，torch默认占用cpu线程数</li>
<li>ray_workers_use_nsight是否开启nsight</li>
<li>VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL 是否使用nccl来编译Ray的 DAG</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>vllm-distribute-inference</div>
      <div>https://tongyangyeyue.github.io/ika-new-blog/2024/11/19/vllm-distribute-inference/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>IKA</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年11月19日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              CC-BY-NC-SA 4.0
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/ika-new-blog/2025/03/18/3fs/" title="3FS">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">3FS</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/ika-new-blog/2024/05/11/kueue/" title="K8s 队列 Kueue">
                        <span class="hidden-mobile">K8s 队列 Kueue</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/ika-new-blog/js/events.js" ></script>
<script  src="/ika-new-blog/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/ika-new-blog/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/ika-new-blog/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/ika-new-blog/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
